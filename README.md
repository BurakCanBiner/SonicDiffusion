# SonicDiffusion: Audio-Driven Image Generation and Editing with Pretrained Diffusion Models

<a href="https://arxiv.org/abs/2405.00878"><img src="https://img.shields.io/badge/arXiv-2307.08397-b31b1b.svg"></a> <a href="https://cyberiada.github.io/SonicDiffusion/"><img src="https://img.shields.io/badge/Project_Page-purple"></a>

>Official Implementation of SonicDiffusion: Audio-Driven Image Generation and Editing with Pretrained Diffusion Models.

<p align="center">
<img src="assets/teaser.png"/>  
</p>

## Updates
**03.05.2024**: Our code and demo are released.

## Demo


## Getting Started

### Prerequisites

```bash
$ conda create -n "myenv" python=3.11
$ git clone git@github.com:BurakCanBiner/SonicDiffusion.git
$ cd SonicDiffusion
$ pip install -r requirements.txt
```

### Pretrained Models

Pretrained models are available at the following link.

https://huggingface.co/spaces/burakcanbiner/SonicDiffusion/tree/main/ckpts


By default, we assume that all models are downloaded and saved to the directory `ckpts`.


## Citation

```
@misc{biner2024sonicdiffusion,
      title={SonicDiffusion: Audio-Driven Image Generation and Editing with Pretrained Diffusion Models}, 
      author={Burak Can Biner and Farrin Marouf Sofian and Umur Berkay Karaka≈ü and Duygu Ceylan and Erkut Erdem and Aykut Erdem},
      year={2024},
      eprint={2405.00878},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
